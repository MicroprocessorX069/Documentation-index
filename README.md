# Documentation Contents
All the documentation content links are placed here

|<h3><a href="https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS/blob/master/documentation/sagan.md">Attention is all you need: Self attention GAN</a></h3>   | |
|:-| :---:|
|<p>How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019. The psuedo implementation is in pytorch.</p> | ![SA gan](https://github.com/MicroprocessorX069/Documentation-index/blob/master/image%20res/sagan%20thumbnail.PNG)  | 


|<h3><a href="https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/cam.md">Class activation mappings: How does your convolutional network see the images.</a></h3>   | |
|:-| :---:|
|<p>Visualizing the hidden representation on a deep conv network. Generating a heat map over the input image to analyse which parts affect the output in Pytorch.</p> |  ![CAM image](https://miro.medium.com/max/516/1*oeKYHc8pNI1DVEqgJ8ERPQ.png)  | 

| <h3><a href="https://github.com/MicroprocessorX069/Generalized-pix2pix-GAN-API/blob/master/documentation/distributed_training.md" target="_blank">Distributed training of network in pytorch</a></h3>  | |
|:-| :---:|
|<p>Understanding the basics of training a network or batch accessing data over a cluster of nodes in Python.</p> |  ![Distributed training](https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS/blob/master/documentation/image%20res/parallelism%20sq.PNG)  | 


| <h3><a href="https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/regularization.md" target="_blank"> Techniques of regularization for a deep neural network</a></h3>  | |
|:-| :---:|
|<p>Why is regularization important? How to control a model from overfitting? This doc discussed various implementations of regularizing a deep neural network.</p> |  ![regularization l2](https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/image%20res/regularization%20sq.PNG)  | 


| <h3><a href="https://github.com/MicroprocessorX069/Generalized-pix2pix-GAN-API/blob/master/documentation/deep_learning_pipelines.md" target="_blank">Design a scalable deep network pipeline supporting hit and trial training</a></h3> | |
|:-| :---:|
|<p>Converting a Pytorch deep network into a scalable pipeline for iterative training and saving the results for the best optimized hyperparametered network.</p> |  ![flowchart](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7sK1NhsM6sXP08IvsCzfMXUigqDDoso4mkEyAO4UXedz0QYMX&s)  | 


## Future additions:

## 5. [Which loss function to use?]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 6. [Frechet inception score: Metric to compare two images for generative tasks]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 8. [Wasserstein loss: What is this?]()

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.
Used in project [Comparison of DC GANS and SA Gans]()

| ## 1. [Spectral Normalization]() 
Improvisation in training
How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch. ||
