# Documentation-index
All the documentation content links are placed here

| <a href="http://fvcproductions.com" target="_blank"><h3> 1. Self attention generative adversarial networks</h3></a>  | |
|:-| :---:|
|<p>How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019. The psuedo implementation is in pytorch.</p> |  ![SA gan](https://github.com/MicroprocessorX069/Documentation-index/blob/master/image%20res/sagan%20thumbnail.PNG)  | 

## 1. [Self attention generative adversarial networks]
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 2. [Techniques of regularization for a deep neural network]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 3. [Class activation mappings: How does your convolutional network see the images.]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 4. [Distributed training of network in pytorch]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 5. [Which loss function to use?]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 6. [Frechet inception score: Metric to compare two images for generative tasks]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.
## 7. [d6tflow integration: Design a scalable deep network pipeline supporting hit and trial training]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 8. [Wasserstein loss: What is this?]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.
Used in project [Comparison of DC GANS and SA Gans]()

| ## 1. [Spectral Normalization]() 
Improvisation in training
How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch. ||
