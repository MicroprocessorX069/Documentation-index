# Documentation Contents
All the documentation content links are placed here

|<h3><a href="https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS/blob/master/documentation/sagan.md">Attention is all you need: Self attention GAN</a></h3>   | |
|:-| :---:|
|<p>How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019. The psuedo implementation is in pytorch.</p> | ![SA gan](https://github.com/MicroprocessorX069/Documentation-index/blob/master/image%20res/sagan%20thumbnail.PNG)  | 


|<h3><a href="https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/cam.md">Class activation mappings: How does your convolutional network see the images.</a></h3>   | |
|:-| :---:|
|<p>How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019. The psuedo implementation is in pytorch.</p> |  ![CAM image](https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/image%20res/cam.png)  | 

| <h3><a href="https://github.com/MicroprocessorX069/Generalized-pix2pix-GAN-API/blob/master/documentation/distributed_training.md" target="_blank">Distributed training of network in pytorch</a></h3>  | |
|:-| :---:|
|<p>How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019. The psuedo implementation is in pytorch.</p> |  ![Distributed training](https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS/blob/master/documentation/image%20res/parallelism%20sq.PNG)  | 


| <h3><a href="https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/regularization.md" target="_blank"> Techniques of regularization for a deep neural network</a></h3>  | |
|:-| :---:|
|<p>Why is regularization important? How to control a model from overfitting? This doc discussed various implementations of regularizing a deep neural network.</p> |  ![regularization l2](https://github.com/MicroprocessorX069/Pneumonia-detection-Dense-Conv-Net/blob/master/documentation/image%20res/regularization%20sq.PNG)  | 


| <h3><a href="https://github.com/MicroprocessorX069/Generalized-pix2pix-GAN-API/blob/master/documentation/deep_learning_pipelines.md" target="_blank">Design a scalable deep network pipeline supporting hit and trial training</a></h3> | |
|:-| :---:|
|<p>How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019. The psuedo implementation is in pytorch.</p> |  ![flowchart](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7sK1NhsM6sXP08IvsCzfMXUigqDDoso4mkEyAO4UXedz0QYMX&s)  | 

## 5. [Which loss function to use?]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 6. [Frechet inception score: Metric to compare two images for generative tasks]()
Improvisation in training

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.

## 8. [Wasserstein loss: What is this?]()

How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch.
Used in project [Comparison of DC GANS and SA Gans]()

| ## 1. [Spectral Normalization]() 
Improvisation in training
How can long range dependencies be handled well among multiple layers? Here is the mechanism of self attention GANS proposed in June 2019.
The psuedo implementation is in pytorch. ||
